# ğŸš€ Serverless GCP Deployment - No SSH Required!

Deploy the Petroleum RAG Benchmark to Google Cloud Platform as a **fully serverless, auto-starting** application!

## âš¡ One-Command Deploy

```bash
./deploy_cloudrun_serverless.sh --project YOUR_PROJECT_ID
```

**That's it!** No VMs to manage, no SSH required, auto-starts when you upload documents!

**Time:** ~15-20 minutes
**Cost:** ~$55/month base + usage (**57% cheaper than VM approach!**)

---

## ğŸ¯ Why Serverless?

### Problems with VM Approach
- âŒ Requires SSH to manage
- âŒ VM runs 24/7 (costs ~$128/month)
- âŒ Manual restart after document upload
- âŒ Must manage Docker containers
- âŒ Requires VM maintenance

### Serverless Benefits
- âœ… **No SSH required ever**
- âœ… **Auto-starts on document upload**
- âœ… **Scales to zero when idle** (save money!)
- âœ… **Fully managed** (no Docker/VM to maintain)
- âœ… **Event-driven** (upload â†’ process â†’ results)
- âœ… **57% cost savings** ($55/month vs $128/month)

---

## ğŸ“‹ Prerequisites

### 1. GCP Account
- Active GCP account with billing enabled
- Project created (or script can prompt you)

### 2. gcloud CLI Installed

**macOS/Linux:**
```bash
curl https://sdk.cloud.google.com | bash
exec -l $SHELL
```

**Authenticate:**
```bash
gcloud auth login
```

### 3. API Keys Ready
- âœ… **Anthropic API key** (sk-ant-...)
- âœ… **OpenAI API key** (sk-...)
- âœ… **LlamaParse API key** (llx-...)

---

## ğŸ—ï¸ What Gets Deployed

```
Google Cloud Platform (Serverless)
â”œâ”€â”€ Cloud Run Service (Streamlit UI)
â”‚   â”œâ”€â”€ Auto-scaling (0-10 instances)
â”‚   â”œâ”€â”€ Scales to zero when idle
â”‚   â””â”€â”€ Always accessible via HTTPS URL
â”‚
â”œâ”€â”€ Cloud Run Job (Processing)
â”‚   â”œâ”€â”€ Triggered automatically on upload
â”‚   â”œâ”€â”€ Or triggered manually
â”‚   â””â”€â”€ Runs only when needed
â”‚
â”œâ”€â”€ Cloud Functions (Auto-Trigger)
â”‚   â”œâ”€â”€ Detects document uploads
â”‚   â””â”€â”€ Triggers processing job
â”‚
â”œâ”€â”€ Memorystore Redis (FalkorDB)
â”‚   â”œâ”€â”€ Managed Redis instance
â”‚   â””â”€â”€ Graph database storage
â”‚
â”œâ”€â”€ Cloud Storage Bucket
â”‚   â”œâ”€â”€ /input/  (Upload documents here)
â”‚   â”œâ”€â”€ /results/ (Benchmark results)
â”‚   â””â”€â”€ /cache/  (Embeddings cache)
â”‚
â”œâ”€â”€ Secret Manager
â”‚   â”œâ”€â”€ anthropic-api-key
â”‚   â”œâ”€â”€ openai-api-key
â”‚   â””â”€â”€ llama-cloud-api-key
â”‚
â””â”€â”€ Eventarc (Event Routing)
    â””â”€â”€ GCS upload â†’ Cloud Function â†’ Cloud Run Job
```

---

## ğŸš€ Deployment Steps

### Step 1: Deploy

```bash
./deploy_cloudrun_serverless.sh --project my-gcp-project
```

The script will:
1. âœ… Enable required APIs (Cloud Run, Cloud Functions, Eventarc, etc.)
2. âœ… Store your API keys in Secret Manager
3. âœ… Create Cloud Storage bucket
4. âœ… Create Memorystore Redis instance (~5 min)
5. âœ… Build container images for UI and processing
6. âœ… Deploy Cloud Run services
7. âœ… Set up auto-trigger function
8. âœ… Provide access URLs

### Step 2: Upload a Document

**Upload triggers processing automatically:**

```bash
# Upload document to input directory
gsutil cp your-document.pdf gs://YOUR_PROJECT-petroleum-rag/input/

# That's it! Processing starts automatically
```

### Step 3: Access the UI

```bash
# Open the URL provided after deployment
# Example: https://petroleum-rag-ui-abc123-uc.a.run.app
```

---

## ğŸ”„ How It Works (Event-Driven Architecture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Upload Document                                          â”‚
â”‚     gsutil cp doc.pdf gs://bucket/input/                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Cloud Function Detects Upload (Eventarc)                â”‚
â”‚     â€¢ Triggered by GCS object finalized event                â”‚
â”‚     â€¢ Validates file is PDF in input/ directory              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Cloud Run Job Triggered                                  â”‚
â”‚     â€¢ Container spins up on-demand                           â”‚
â”‚     â€¢ Runs benchmark.py automatically                        â”‚
â”‚     â€¢ 4 parsers Ã— 3 storage = 12 combinations                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Results Saved to Cloud Storage                          â”‚
â”‚     â€¢ Benchmark results â†’ gs://bucket/results/               â”‚
â”‚     â€¢ Cached embeddings â†’ gs://bucket/cache/                 â”‚
â”‚     â€¢ Job completes, container shuts down                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. View Results in UI                                       â”‚
â”‚     â€¢ Access Cloud Run UI URL                                â”‚
â”‚     â€¢ See benchmark results                                  â”‚
â”‚     â€¢ Ask questions in chat                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**No SSH required at any step!**

---

## ğŸ’° Cost Breakdown

### Monthly Costs (Serverless)

| Service | Always On? | Monthly Cost |
|---------|-----------|--------------|
| Memorystore Redis (5GB) | âœ… Yes | ~$50 |
| Cloud Storage (100GB) | âœ… Yes | ~$2 |
| Secret Manager | âœ… Yes | ~$1 |
| Cloud Run UI (idle most of time) | âš ï¸ Scales to 0 | ~$1 |
| Cloud Run Job (per run) | âŒ No | ~$0.50/run |
| Cloud Functions (triggers) | âŒ No | ~$0.10/month |
| Processing APIs (LLM, embeddings) | âŒ No | ~$2.50/run |
| **Total Base Cost** | | **~$55/month** |
| **Per Processing Run** | | **~$3/run** |

### Compare to VM Approach

| Metric | VM Approach | Serverless | Savings |
|--------|-------------|------------|---------|
| Base monthly cost | $128 | $55 | **57%** |
| Requires SSH | âœ… Yes | âŒ No | âœ… |
| Manual start | âœ… Yes | âŒ Auto | âœ… |
| Scales to zero | âŒ No | âœ… Yes | âœ… |
| Maintenance | âš ï¸ Docker/VM | âœ… None | âœ… |

**Serverless wins on cost AND convenience!**

---

## ğŸ“Š Usage Examples

### Auto-Processing Workflow

```bash
# 1. Upload document (triggers processing automatically)
gsutil cp handbook.pdf gs://my-project-petroleum-rag/input/

# 2. Check processing logs
gcloud logging read 'resource.type=cloud_run_job' --limit 50

# 3. View UI when ready
open https://petroleum-rag-ui-abc123-uc.a.run.app
```

### Manual Processing Trigger

```bash
# Trigger processing manually (without uploading document)
gcloud run jobs execute petroleum-rag-processor --region=us-central1

# View job execution history
gcloud run jobs executions list --region=us-central1
```

### Upload Multiple Documents

```bash
# Upload all PDFs from local directory
gsutil -m cp data/input/*.pdf gs://my-project-petroleum-rag/input/

# Each file triggers a separate processing job automatically
```

---

## ğŸ”§ Management Commands

### View Processing Logs

```bash
# View recent processing logs
gcloud logging read 'resource.type=cloud_run_job' --limit 50 --format json

# Follow logs in real-time
gcloud logging tail 'resource.type=cloud_run_job'
```

### View Job Executions

```bash
# List all job executions
gcloud run jobs executions list \
  --job=petroleum-rag-processor \
  --region=us-central1

# Get details of specific execution
gcloud run jobs executions describe EXECUTION_ID \
  --region=us-central1
```

### Access UI Service

```bash
# Get UI URL
gcloud run services describe petroleum-rag-ui \
  --region=us-central1 \
  --format='value(status.url)'

# View UI logs
gcloud run services logs read petroleum-rag-ui \
  --region=us-central1 \
  --limit=50
```

### Manage Cloud Storage

```bash
# List documents
gsutil ls gs://my-project-petroleum-rag/input/

# Download results
gsutil -m cp -r gs://my-project-petroleum-rag/results/ ./local-results/

# View cache statistics
gsutil du -sh gs://my-project-petroleum-rag/cache/
```

### Check Redis Instance

```bash
# Get Redis connection info
gcloud redis instances describe petroleum-rag-redis \
  --region=us-central1

# View Redis metrics
gcloud monitoring time-series list \
  --filter='resource.type="redis_instance"'
```

---

## âš™ï¸ Optional: Scheduled Processing

Set up daily automatic processing:

```bash
# The deployment script asks if you want this
# Or set up manually:

# Schedule daily processing at 2 AM
gcloud scheduler jobs create http petroleum-rag-daily \
  --location=us-central1 \
  --schedule="0 2 * * *" \
  --uri="https://us-central1-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/PROJECT_ID/jobs/petroleum-rag-processor:run" \
  --http-method=POST \
  --oauth-service-account-email=cloud-run-invoker@PROJECT_ID.iam.gserviceaccount.com
```

---

## ğŸ” Troubleshooting

### Processing Not Starting After Upload

**Check Cloud Function logs:**
```bash
gcloud functions logs read petroleum-rag-trigger \
  --region=us-central1 \
  --limit=50
```

**Verify Eventarc trigger:**
```bash
gcloud eventarc triggers list --location=us-central1
```

**Manual trigger to test:**
```bash
gcloud run jobs execute petroleum-rag-processor --region=us-central1
```

### UI Not Accessible

**Check service status:**
```bash
gcloud run services describe petroleum-rag-ui --region=us-central1
```

**View logs:**
```bash
gcloud run services logs read petroleum-rag-ui --region=us-central1
```

**Redeploy if needed:**
```bash
gcloud run services update petroleum-rag-ui \
  --region=us-central1 \
  --image=gcr.io/PROJECT_ID/petroleum-rag-ui
```

### Redis Connection Issues

**Check Redis status:**
```bash
gcloud redis instances describe petroleum-rag-redis --region=us-central1
```

**Verify network connectivity:**
- Cloud Run services must be in same VPC as Redis
- Check VPC connector is attached to Cloud Run services

### Secret Access Issues

**Verify secrets exist:**
```bash
gcloud secrets list
```

**Check IAM permissions:**
```bash
gcloud run services get-iam-policy petroleum-rag-ui --region=us-central1
```

**Grant secret access if needed:**
```bash
gcloud secrets add-iam-policy-binding anthropic-api-key \
  --member="serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com" \
  --role="roles/secretmanager.secretAccessor"
```

---

## ğŸ“ˆ Scaling and Performance

### UI Auto-Scaling

```bash
# Update max instances (default: 10)
gcloud run services update petroleum-rag-ui \
  --region=us-central1 \
  --max-instances=20

# Set minimum instances (avoid cold starts)
gcloud run services update petroleum-rag-ui \
  --region=us-central1 \
  --min-instances=1  # Costs more but no cold start
```

### Processing Job Resources

```bash
# Increase memory/CPU for large documents
gcloud run jobs update petroleum-rag-processor \
  --region=us-central1 \
  --memory=16Gi \
  --cpu=8
```

### Redis Scaling

```bash
# Increase Redis memory
gcloud redis instances update petroleum-rag-redis \
  --region=us-central1 \
  --size=10  # GB
```

---

## ğŸ—‘ï¸ Clean Up / Delete Everything

```bash
# Delete Cloud Run services and jobs
gcloud run services delete petroleum-rag-ui --region=us-central1 --quiet
gcloud run jobs delete petroleum-rag-processor --region=us-central1 --quiet

# Delete Cloud Function
gcloud functions delete petroleum-rag-trigger --region=us-central1 --quiet

# Delete Redis instance
gcloud redis instances delete petroleum-rag-redis --region=us-central1 --quiet

# Delete Cloud Storage bucket
gsutil -m rm -r gs://PROJECT_ID-petroleum-rag

# Delete secrets
gcloud secrets delete anthropic-api-key --quiet
gcloud secrets delete openai-api-key --quiet
gcloud secrets delete llama-cloud-api-key --quiet

# Delete Cloud Scheduler jobs (if created)
gcloud scheduler jobs delete petroleum-rag-daily --location=us-central1 --quiet
```

**âš ï¸ Warning:** This permanently deletes all data and configurations!

---

## ğŸ”’ Security Best Practices

### 1. Restrict UI Access

```bash
# Require authentication
gcloud run services update petroleum-rag-ui \
  --region=us-central1 \
  --no-allow-unauthenticated

# Access with:
gcloud run services proxy petroleum-rag-ui --port=8080
# Then open: http://localhost:8080
```

### 2. Use VPC for Redis

```bash
# Create VPC connector (already done by deploy script)
gcloud compute networks vpc-access connectors create petroleum-rag-connector \
  --region=us-central1 \
  --network=default \
  --range=10.8.0.0/28

# Attach to Cloud Run
gcloud run services update petroleum-rag-ui \
  --region=us-central1 \
  --vpc-connector=petroleum-rag-connector
```

### 3. Audit Logs

```bash
# Enable audit logging
gcloud logging read "resource.type=cloud_run_revision" --limit=100

# Set up log-based alerts
gcloud logging metrics create error-rate \
  --description="Error rate in Cloud Run" \
  --log-filter='resource.type="cloud_run_revision" AND severity="ERROR"'
```

---

## ğŸ“š Architecture Comparison

### VM Approach (Old)
```
User â†’ [SSH to VM] â†’ [Start Docker] â†’ [Run benchmark.py] â†’ [View results]
      Manual steps required at every stage
```

### Serverless Approach (New)
```
User â†’ [Upload to GCS] â†’ [Auto-trigger] â†’ [Auto-process] â†’ [View results]
      Fully automated, no SSH required
```

---

## âœ… Quick Checklist

**Before deploying:**
- [ ] GCP account with billing enabled
- [ ] gcloud CLI installed and authenticated
- [ ] API keys ready (Anthropic, OpenAI, LlamaParse)
- [ ] Project ID chosen

**After deploying:**
- [ ] Cloud Run services are running
- [ ] Can access Streamlit UI URL
- [ ] Uploaded test document to GCS
- [ ] Processing triggered automatically
- [ ] Results visible in UI
- [ ] Monitoring logs accessible

---

## ğŸ†˜ Need Help?

1. **Check deployment info:**
   ```bash
   cat deployment-serverless-info.txt
   ```

2. **View logs:**
   ```bash
   gcloud logging read 'resource.type=cloud_run_job' --limit 50
   ```

3. **Test trigger manually:**
   ```bash
   gcloud run jobs execute petroleum-rag-processor --region=us-central1
   ```

---

## ğŸ‰ Ready to Deploy?

```bash
./deploy_cloudrun_serverless.sh --project YOUR_PROJECT_ID
```

**In ~20 minutes you'll have:**
- âœ… Fully serverless RAG system
- âœ… Auto-processing on document upload
- âœ… No SSH or VM management
- âœ… 57% cost savings vs VM approach
- âœ… Scales to zero when idle

**Upload a document and watch it process automatically!** ğŸš€
